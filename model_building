# Importing the necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error

#I have considered the sample data 
sample_data = {
    'Traffic': [3, 2, 1, 4, 3],
    'AQI': [20, 30, 15, 25, 18],
    'Green_Cover': [60, 70, 80, 50, 65],
    'Cleanliness_Score': [90, 85, 95, 80, 88],
}

sd= pd.DataFrame(data)

#  I have used the hyper tuning parameter called Feature engineering
sd['Traffic_Green_Cover'] = sd['Traffic'] * sd['Green_Cover']

# Splitting  the data into features and target variable
x = sd[['Traffic', 'AQI', 'Green_Cover', 'Traffic_Green_Cover']]
y = sd['Cleanliness_Score']

# Splitting the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Building a  model using Random Forest Regressor  
model = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=42))

# Here  hyperparameters for tuning is defined
param_grid = {
    'randomforestregressor__n_estimators': [50, 100, 150],
    'randomforestregressor__max_depth': [None, 10, 20, 30],
}

# Performing  GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error')
grid_search.fit(x_train, y_train)

# Fetching the best model from GridSearchCV
best_model = grid_search.best_estimator_

# Making the  predictions on the test set
predictions = best_model.predict(x_test)

# Evaluating  the model
mse = mean_squared_error(y_test, predictions)
print(f'Best Model - Mean Squared Error: {mse}')
print(f'Best Hyperparameters: {grid_search.best_params_}')


